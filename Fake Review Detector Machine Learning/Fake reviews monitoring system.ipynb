{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn.linear_model._logistic\n",
    "import sys\n",
    "import re\n",
    "import pickle\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.modules['sklearn.linear_model.logistic'] = sklearn.linear_model._logistic\n",
    "dataset = pd.read_csv(\"reviews.csv\",sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dataset))\n",
    "dataset.dropna(subset = ['review_date'], inplace=True)\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset[:1000]\n",
    "dataset = dataset.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"Pickle Files/classifier.pickle\",\"rb\") as f:\n",
    "    clf = pickle.load(f)\n",
    "\n",
    "with open(\"Pickle Files/TfidfModel.pickle\",\"rb\") as f:\n",
    "    tfidf = pickle.load(f)\n",
    "\n",
    "def getSentiment(text):\n",
    "\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"that's\",\"that is\",text)\n",
    "    text = re.sub(r\"there's\",\"there is\",text)\n",
    "    text = re.sub(r\"what's\",\"what is\",text)\n",
    "    text = re.sub(r\"where's\",\"where is\",text)\n",
    "    text = re.sub(r\"it's\",\"it is\",text)\n",
    "    text = re.sub(r\"who's\",\"who is\",text)\n",
    "    text = re.sub(r\"i'm\",\"i am\",text)\n",
    "    text = re.sub(r\"she's\",\"she is\",text)\n",
    "    text = re.sub(r\"he's\",\"he is\",text)\n",
    "    text = re.sub(r\"they're\",\"they are\",text)\n",
    "    text = re.sub(r\"who're\",\"who are\",text)\n",
    "    text = re.sub(r\"ain't\",\"am not\",text)\n",
    "    text = re.sub(r\"wouldn't\",\"would not\",text)\n",
    "    text = re.sub(r\"shouldn't\",\"should not\",text)\n",
    "    text = re.sub(r\"can't\",\"can not\",text)\n",
    "    text = re.sub(r\"couldn't\",\"could not\",text)\n",
    "    text = re.sub(r\"won't\",\"will not\",text)\n",
    "    \n",
    "    text = re.sub(r\"\\W\",\" \",text)\n",
    "    text = re.sub(r\"\\d\",\" \",text)\n",
    "    text = re.sub(r\"\\s+[a-z]\\s+\",\" \",text)\n",
    "    text = re.sub(r\"^[a-z]\\s+\",\" \",text)    \n",
    "    text = re.sub(r\"\\s+[a-z]$\",\" \",text)    \n",
    "    text = re.sub(r\"\\s+\",\" \",text)    \n",
    "    sent = clf.predict(tfidf.transform([text]).toarray())\n",
    "   \n",
    "    return sent[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Reviews which have dual view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. different sentiment in review headline and review body\n",
    "\n",
    "remove_reviews = []\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    \n",
    "        if( getSentiment( dataset[\"review_headline\"][i] ) != getSentiment( dataset[\"review_body\"][i] ) ):\n",
    "            \n",
    "            remove_reviews.append(dataset[\"review_id\"][i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(remove_reviews)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Reviews in which same user promoting or demoting a particular brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Users which are posting either all positive or negative reviews on different products of same brand\n",
    "\n",
    "customers = dataset.groupby(\"customer_id\")\n",
    "\n",
    "customer_list = dataset[\"customer_id\"].unique()\n",
    " \n",
    "size = len(customer_list.tolist())\n",
    "\n",
    "for i in range(size):\n",
    "    \n",
    "    brand_df = customers.get_group(customer_list[i])    \n",
    "    \n",
    "    brands = brand_df.groupby(\"product_parent\")\n",
    "    \n",
    "    brands_list = brand_df[\"product_parent\"].unique()\n",
    "    \n",
    "    no_of_brands = len(brands_list.tolist())\n",
    "    \n",
    "    for j in range(no_of_brands):\n",
    "        \n",
    "        product_df = brands.get_group(brands_list[j])\n",
    "        \n",
    "        no_of_products = len(product_df[\"product_id\"])\n",
    "        \n",
    "        if no_of_products<=2:\n",
    "            continue\n",
    "            \n",
    "        indices = product_df.index.values.tolist()\n",
    "        \n",
    "        sentiment = getSentiment(product_df[\"review_body\"][indices[0]])\n",
    "        \n",
    "        isSameSentiment = True\n",
    "        \n",
    "        for k in range(1,no_of_products):\n",
    "            \n",
    "            text = str(product_df[\"review_body\"][indices[k]])\n",
    "            \n",
    "            if getSentiment(text)!=sentiment :\n",
    "                isSameSentiment = False\n",
    "                break\n",
    "                \n",
    "        if(isSameSentiment):\n",
    "            remove_reviews.append(customer_list[i])\n",
    "            break\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(remove_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Reviews in which person from same IP Address promoting or demoting a particular brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.Reviews from same IP either all positive or negative reviews on different products of same brand\n",
    "\n",
    "\n",
    "ip = dataset.groupby(\"IP Address\")\n",
    "\n",
    "ip_list = dataset[\"IP Address\"].unique()\n",
    "\n",
    "remove_ip = []\n",
    "\n",
    "size = len(ip_list.tolist())\n",
    "\n",
    "for i in range(size):\n",
    "    \n",
    "    brand_df = ip.get_group(ip_list[i])\n",
    "    \n",
    "    brands = brand_df.groupby(\"product_parent\")\n",
    "    \n",
    "    brands_list = brand_df[\"product_parent\"].unique()\n",
    "    \n",
    "    no_of_brands = len(brands_list.tolist())\n",
    "    \n",
    "    for j in range(no_of_brands):\n",
    "        \n",
    "        product_df = brands.get_group(brands_list[j])\n",
    "        \n",
    "        no_of_products = len(product_df[\"product_id\"])\n",
    "        \n",
    "        if no_of_products<=2:\n",
    "            break\n",
    "        \n",
    "        indices = product_df.index.tolist()\n",
    "        \n",
    "        sentiment = getSentiment(product_df[\"review_body\"][ indices[0] ])\n",
    "                \n",
    "        isSameSentiment = True\n",
    "        \n",
    "        for k in range(1,no_of_products):\n",
    "            \n",
    "            text = str(product_df[\"review_body\"][indices[k]])\n",
    "            \n",
    "            if getSentiment(text)!=sentiment :\n",
    "                isSameSentiment = False\n",
    "                break\n",
    "                \n",
    "        if(isSameSentiment):\n",
    "            remove_ip.append(ip_list[i])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_ip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Reviews which are posted as flood by same user all the reviews are either positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.sort_values(\"customer_id\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. User posting (>3) reviews on the same day with all the reviews are either positive or negative.\n",
    "\n",
    "customer_group = dataset.groupby(\"customer_id\")\n",
    "\n",
    "customer_group_list = dataset[\"customer_id\"].unique().tolist()\n",
    "\n",
    "for i in range(len(customer_group_list)):\n",
    "    \n",
    "    customer_reviews = customer_group.get_group( customer_group_list[i] )\n",
    "    \n",
    "    dates_list = customer_reviews[\"review_date\"].unique().tolist()\n",
    "    \n",
    "    reviews_by_date = customer_reviews.groupby(\"review_date\")\n",
    "    \n",
    "    for j in range(len(dates_list)):\n",
    "        \n",
    "        reviews_by_date_for_pos = []\n",
    "        reviews_by_date_for_neg = []\n",
    "        \n",
    "        df = reviews_by_date.get_group(dates_list[j])\n",
    "                        \n",
    "        indices = df.index.tolist()\n",
    "        \n",
    "        for k in range(len(df)):\n",
    "            \n",
    "            text = df[\"review_body\"][ indices[k] ]\n",
    "            \n",
    "            if(getSentiment(text) == 0):\n",
    "                reviews_by_date_for_neg.append(df[\"review_id\"][ indices[k] ])\n",
    "            else:\n",
    "                reviews_by_date_for_pos.append(df[\"review_id\"][ indices[k] ])\n",
    "                        \n",
    "        if(len(reviews_by_date_for_pos)>3):\n",
    "            remove_reviews.extend(reviews_by_date_for_pos)\n",
    "        \n",
    "        if(len(reviews_by_date_for_neg)>3):\n",
    "            remove_reviews.extend(reviews_by_date_for_neg)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(remove_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Reviews which are posted as flood by same person from same IP Address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. Reviews(>3) from same IP on the same day with all the reviews are either positive or negative.\n",
    "\n",
    "ip_group = dataset.groupby(\"IP Address\")\n",
    "\n",
    "ip_list = dataset[\"IP Address\"].unique().tolist()\n",
    "\n",
    "size = len(ip_list)\n",
    "\n",
    "for i in range(size):\n",
    "    \n",
    "    reviews = ip_group.get_group( ip_list[i] )\n",
    "    \n",
    "    dates_list = reviews[\"review_date\"].unique().tolist()\n",
    "    \n",
    "    reviews_by_date = reviews.groupby(\"review_date\");\n",
    "    \n",
    "    for j in range(len(dates_list)):\n",
    "        \n",
    "        reviews_by_date_for_pos = []\n",
    "        reviews_by_date_for_neg = []\n",
    "\n",
    "        \n",
    "        reviews_for_each_day = reviews_by_date.get_group(dates_list[j])\n",
    "        \n",
    "        indices = reviews_for_each_day.index.tolist()\n",
    "        \n",
    "        for k in range(len(reviews_for_each_day)):\n",
    "            \n",
    "            text = reviews_for_each_day[\"review_body\"][ indices[k] ]\n",
    "            \n",
    "            if(getSentiment(text) == 0):\n",
    "                reviews_by_date_for_neg.append(reviews_for_each_day[\"review_id\"][ indices[k] ])\n",
    "            else:   \n",
    "                reviews_by_date_for_pos.append(reviews_for_each_day[\"review_id\"][ indices[k] ])\n",
    "                     \n",
    "        if(len(reviews_by_date_for_pos)>3):\n",
    "            remove_reviews.extend(reviews_by_date_for_pos)\n",
    "        \n",
    "        if(len(reviews_by_date_for_neg)>3):\n",
    "            remove_reviews.extend(reviews_by_date_for_neg)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(remove_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Similar reviews posted in the same time interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.reset_index()\n",
    "dataset.set_index(\"review_id\")\n",
    "dataset.sort_values(\"timestamp\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "remove_reviews2 = []\n",
    "indices = []\n",
    "for i in range(len(dataset)):\n",
    "    \n",
    "    reviews = [str(dataset[\"review_body\"][i])]\n",
    "    \n",
    "    try:\n",
    "        tfidf.transform(reviews)\n",
    "    except:\n",
    "        # reviews with one word and with no dictionary meaning will be invalid\n",
    "        remove_reviews2.append(dataset[\"review_id\"][i]) \n",
    "        continue\n",
    "        \n",
    "    Time = dataset[\"timestamp\"][i]\n",
    "    \n",
    "    for j in range(i+1,len(dataset)):\n",
    "            \n",
    "        indices.append(dataset[\"review_id\"][j])\n",
    "        \n",
    "        if(dataset[\"timestamp\"][j]-Time <= 1800):\n",
    "            reviews.append(str(dataset[\"review_body\"][j]))\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    tfidf_matrix = tfidf.transform(reviews)\n",
    "    tfidf_list = cosine_similarity(tfidf_matrix).tolist()\n",
    "         \n",
    "    i_appended = False\n",
    "    for k in range(1,len(tfidf_list[0])):\n",
    "        \n",
    "        if(tfidf_list[0][k]>0.6):\n",
    "            \n",
    "            remove_reviews2.append(dataset[\"review_id\"][i+k])\n",
    "            \n",
    "            if(not i_appended):\n",
    "                remove_reviews2.append(dataset[\"review_id\"][i]) \n",
    "                i_appended = True\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(remove_reviews2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Reviews in which Reviewer using arming tone to by the product (Action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7. Removing reviews with no. of verbs > no. of nouns\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    \n",
    "    words = nltk.word_tokenize(str(dataset[\"review_body\"][i]))\n",
    "    \n",
    "    tagged_words = nltk.pos_tag(words)\n",
    "    \n",
    "    nouns_count = 0\n",
    "    verbs_count = 0\n",
    "    \n",
    "    for j in range(len(tagged_words)):\n",
    "\n",
    "        if(tagged_words[j][1].startswith(\"NN\")):\n",
    "            nouns_count+=1\n",
    "            #counts the no. of nouns in the review\n",
    "\n",
    "        if(tagged_words[j][1].startswith(\"VB\")):\n",
    "            verbs_count+=1\n",
    "            #counts the no. of verbs in the review\n",
    "\n",
    "    if(verbs_count>nouns_count):\n",
    "        remove_reviews.append(dataset[\"review_id\"][i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(remove_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Reviews in which reviewer is writing his own story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Removing reviews with includes more first person pronouns.\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    \n",
    "    dataset[\"review_body\"][i] = str(dataset[\"review_body\"][i]).lower()\n",
    "    \n",
    "    words = nltk.word_tokenize(dataset[\"review_body\"][i])\n",
    "    \n",
    "    sentence = nltk.sent_tokenize(dataset[\"review_body\"][i])\n",
    "    \n",
    "    count=0\n",
    "    if(len(sentence)>4):\n",
    "        \n",
    "        for j in range(len(words)):\n",
    "\n",
    "            if(words[j]==\"i\" or words[j]==\"we\"):\n",
    "                count+=1\n",
    "                \n",
    "        if(count > len(sentence)/2):\n",
    "            remove_reviews.append(dataset[\"review_id\"][i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(remove_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Meaningless Texts in reviews using LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.set_index(\"review_id\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latent symantic analysis\n",
    "# it will analyse all reviews and determine all reviews belong to the same concept\n",
    "def LSA(text):\n",
    "    \n",
    "    X = tfidf.transform(text)\n",
    "    \n",
    "    lsa = TruncatedSVD(n_components = 1,n_iter = 100)\n",
    "    lsa.fit(X)\n",
    "    \n",
    "    terms = tfidf.get_feature_names()\n",
    "    concept_words={}\n",
    "\n",
    "    for j,comp in enumerate(lsa.components_):\n",
    "        componentTerms = zip(terms,comp)\n",
    "        sortedTerms = sorted(componentTerms,key=lambda x:x[1],reverse=True)\n",
    "        sortedTerms = sortedTerms[:10]\n",
    "        concept_words[str(j)] = sortedTerms\n",
    "     \n",
    "    sentence_scores = []\n",
    "    for key in concept_words.keys():\n",
    "        for sentence in text:\n",
    "            words = nltk.word_tokenize(sentence)\n",
    "            scores = 0\n",
    "            for word in words:\n",
    "                for word_with_scores in concept_words[key]:\n",
    "                    if word == word_with_scores[0]:\n",
    "                        scores += word_with_scores[1]\n",
    "            sentence_scores.append(scores)\n",
    "    return sentence_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_df = dataset.groupby(\"product_id\")\n",
    "\n",
    "unique_products = dataset[\"product_id\"].unique()\n",
    "\n",
    "no_products = len(unique_products.tolist())\n",
    "\n",
    "remove_reviews3 = []\n",
    "    \n",
    "for i in range(no_products):\n",
    "    \n",
    "    df = product_df.get_group(unique_products[i])\n",
    "    \n",
    "    unique_reviews = df.index.tolist()\n",
    "    \n",
    "    no_reviews = len(unique_reviews)   \n",
    "    \n",
    "    count = no_reviews \n",
    "    \n",
    "    reviews = []\n",
    "    \n",
    "    review_id = []\n",
    "    \n",
    "    for j in range(no_reviews):\n",
    "        \n",
    "        text = str(df.loc[unique_reviews[j]][\"review_body\"])\n",
    "        \n",
    "        #cleaning the text\n",
    "        text = re.sub(r\"\\W\",\" \",text)             \n",
    "        text = re.sub(r\"\\d\",\" \",text)             \n",
    "        text = re.sub(r\"\\s+[a-z]\\s+\",\" \",text)    \n",
    "        text = re.sub(r\"^[a-z]\\s+\",\" \",text)    \n",
    "        text = re.sub(r\"\\s+[a-z]$\",\" \",text)    \n",
    "        text = re.sub(r\"\\s+\",\" \",text)\n",
    "        \n",
    "        words = nltk.word_tokenize(text)\n",
    "        \n",
    "        if(len(words)==1):\n",
    "            \n",
    "            if(len(text) <=1 or not wordnet.synsets(text) ):\n",
    "            #if word is having only one character or invalid english word\n",
    "                \n",
    "                remove_reviews3.append(unique_reviews[j])\n",
    "                count-=1\n",
    "                continue\n",
    "                \n",
    "        elif(len(words)==0):\n",
    "            \n",
    "            remove_reviews3.append(unique_reviews[j])\n",
    "            count-=1\n",
    "            continue\n",
    "        \n",
    "        review_id.append(unique_reviews[j])        \n",
    "        reviews.append(text)\n",
    "        \n",
    "    if(count<=0):\n",
    "        continue\n",
    "        \n",
    "    if(count==1):                 \n",
    "        text = [text,str(df.loc[review_id[0]][\"product_title\"])]         \n",
    "        sentence_scores = LSA(text) \n",
    "        \n",
    "        if(sentence_scores[0]==0): \n",
    "            remove_reviews3.append(review_id[0])\n",
    "        continue\n",
    "    \n",
    "    sentence_scores = LSA(reviews)\n",
    "            \n",
    "    for j in range(len(sentence_scores)):        \n",
    "        if(sentence_scores[j]==0.00):\n",
    "            remove_reviews3.append(review_id[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(remove_reviews3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Fake Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.drop(remove_reviews2,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    dataset.drop(remove_reviews3,inplace=True)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    dataset.drop(remove_reviews,inplace=True)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.set_index(\"IP Address\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.drop(remove_ip,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv(\"real_reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ddd5997caa9b7694450397d81f5d1362c20fdf2d7901afab31efb88429cb4b94"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
